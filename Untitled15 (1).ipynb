{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "afb545330b394de59a741ea88adccfa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f42ccb0d43444e3b911e74e9de4d95e5",
              "IPY_MODEL_ad0fd65c756142519ee96df58858d5b9",
              "IPY_MODEL_7368ecc3c7fc4719bc504c771ef2679e"
            ],
            "layout": "IPY_MODEL_83efd2ffff2d4e76acd9b5f7a572a93f"
          }
        },
        "f42ccb0d43444e3b911e74e9de4d95e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_509f4f90c06a46189184e183105377fc",
            "placeholder": "​",
            "style": "IPY_MODEL_2423239d23784d0c88bbe514bbf76c70",
            "value": "model.safetensors: 100%"
          }
        },
        "ad0fd65c756142519ee96df58858d5b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fa18876ceea4d3fae6ed7c23adcc812",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_716ac9a279e645ea9c6d6d3c053ec419",
            "value": 440449768
          }
        },
        "7368ecc3c7fc4719bc504c771ef2679e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3483d468a32240f5ba1c490d6974ecc1",
            "placeholder": "​",
            "style": "IPY_MODEL_b6b403067aa94d5c882ff49b7675009c",
            "value": " 440M/440M [00:01&lt;00:00, 340MB/s]"
          }
        },
        "83efd2ffff2d4e76acd9b5f7a572a93f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "509f4f90c06a46189184e183105377fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2423239d23784d0c88bbe514bbf76c70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fa18876ceea4d3fae6ed7c23adcc812": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "716ac9a279e645ea9c6d6d3c053ec419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3483d468a32240f5ba1c490d6974ecc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6b403067aa94d5c882ff49b7675009c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmMGl_8cGUW_",
        "outputId": "c8687fee-f79e-4f89-d8a0-d61444255994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForMaskedLM\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install NLTK resources\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# Set the path to your dataset in Google Drive\n",
        "dataset_path = '/content/drive/MyDrive/archive (4)/recipe_dataset.csv'\n",
        "\n",
        "# Read the CSV file with the specified columns\n",
        "recipes = pd.read_csv(dataset_path, usecols=['name', 'description', 'cuisine', 'course', 'diet', 'ingredients_name', 'ingredients_quantity', 'prep_time (in mins)', 'cook_time (in mins)', 'instructions', 'image_url'])\n",
        "\n",
        "# Preprocessing ingredients and instructions\n",
        "recipes['ingredients_name'] = recipes['ingredients_name'].str.lower().str.replace(\"[^a-z\\s]+\", \" \").str.replace(\"(\\s+|$\\s+|\\s+^)\", \" \")\n",
        "\n",
        "# Lemmatize ingredients\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "recipes['ingredients_name'] = recipes['ingredients_name'].fillna('').apply(lambda row: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(row)]))\n",
        "\n",
        "# Create the DataFrame with special tokens\n",
        "df = \"<RECIPE_START> <INPUT_START> <INGREDIENTS_START> \" + recipes['ingredients_name'] + \" <INPUT_END> <INSTRUCTIONS_START> \" + \\\n",
        "    recipes['instructions'] + \" <INSTRUCTIONS_END> <TITLE_START> \" + recipes['name'] + \" <TITLE_END>\"\n",
        "\n",
        "# Split the dataset into train and test\n",
        "train, test = train_test_split(df, test_size=0.05)\n",
        "\n",
        "# Set the path to save the generated data\n",
        "output_dir = '/content/drive/My Drive/generated_data/'\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save ingredients and recipes as text files\n",
        "np.savetxt(os.path.join(output_dir, 'ingredients.txt'), recipes['ingredients_name'], fmt='%s')\n",
        "np.savetxt(os.path.join(output_dir, 'recipes.txt'), recipes['instructions'], fmt='%s')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIPGwL40G744",
        "outputId": "956d99f5-809e-4d23-a932-70ceb445ba56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_series = train.squeeze()\n",
        "test_series = test.squeeze()\n",
        "# Tokenize and encode the input sequences\n",
        "input_sequences = [tokenizer.encode(seq, add_special_tokens=True, max_length=512, truncation=True, padding='max_length') for seq in train_series.str.split('<INPUT_END>').str[0]]\n",
        "target_sequences = [tokenizer.encode(seq, add_special_tokens=True, max_length=512, truncation=True, padding='max_length') for seq in train_series.str.split('<INSTRUCTIONS_START>').str[1].str.split('<INSTRUCTIONS_END>').str[0]]"
      ],
      "metadata": {
        "id": "RT8HjHiaHNvO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((input_sequences, target_sequences)).batch(16)\n",
        "\n",
        "# Load the BERT model\n",
        "model = TFBertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define the training loop\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "afb545330b394de59a741ea88adccfa3",
            "f42ccb0d43444e3b911e74e9de4d95e5",
            "ad0fd65c756142519ee96df58858d5b9",
            "7368ecc3c7fc4719bc504c771ef2679e",
            "83efd2ffff2d4e76acd9b5f7a572a93f",
            "509f4f90c06a46189184e183105377fc",
            "2423239d23784d0c88bbe514bbf76c70",
            "2fa18876ceea4d3fae6ed7c23adcc812",
            "716ac9a279e645ea9c6d6d3c053ec419",
            "3483d468a32240f5ba1c490d6974ecc1",
            "b6b403067aa94d5c882ff49b7675009c"
          ]
        },
        "id": "EFvCLE1fNJnX",
        "outputId": "a3c34735-a83b-46bf-ce96-4fbc8b98accb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afb545330b394de59a741ea88adccfa3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(input_ids, target_ids):\n",
        "    with tf.GradientTape() as tape:\n",
        "        outputs = model(input_ids, labels=target_ids)\n",
        "        loss = outputs.loss\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "r2emYZd8NO-C"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for input_ids, target_ids in train_dataset:\n",
        "        loss = train_step(input_ids, target_ids)\n",
        "        total_loss += loss\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uglXaC8FNXm0",
        "outputId": "8a72f7ad-2b7a-4a97-e90c-d12c1dce3391"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: [1683.3115]\n",
            "Epoch 2, Loss: [1501.5773]\n",
            "Epoch 3, Loss: [1474.6028]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "model.save_pretrained('/content/drive/My Drive/generated_data/bert_model')"
      ],
      "metadata": {
        "id": "5SPrkPSBRP7_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertLMHeadModel\n",
        "\n",
        "# Load the saved BERT model\n",
        "model = TFBertForMaskedLM.from_pretrained('/content/drive/My Drive/generated_data/bert_model')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to preprocess the input ingredients\n",
        "def preprocess_input(ingredients):\n",
        "    input_text = \"<RECIPE_START> <INPUT_START> <INGREDIENTS_START> \" + ingredients + \" <INPUT_END>\"\n",
        "    input_ids = tokenizer.encode(input_text, add_special_tokens=True, max_length=512, truncation=True, padding='max_length')\n",
        "    return input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEx-WjzKRRzS",
        "outputId": "e64d097e-cd56-4e13-e59a-fdd7c9951894"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at /content/drive/My Drive/generated_data/bert_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate instructions\n",
        "def generate_instructions(ingredients):\n",
        "    input_ids = preprocess_input(ingredients)\n",
        "    input_tensor = tf.constant([input_ids])\n",
        "\n",
        "    output = model.generate(input_tensor, max_length=512, do_sample=True, top_k=50, top_p=0.95, num_return_sequences=1)\n",
        "\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    instructions = generated_text.split('<INSTRUCTIONS_START>')[1].split('<INSTRUCTIONS_END>')[0]\n",
        "\n",
        "    return instructions"
      ],
      "metadata": {
        "id": "9w_Da6hERaeC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertLMHeadModel\n",
        "\n",
        "# Load the saved BERT model\n",
        "model = TFBertLMHeadModel.from_pretrained('/content/drive/My Drive/generated_data/bert_model')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',padding_side='left')\n",
        "\n",
        "# Function to preprocess the input ingredients\n",
        "def preprocess_input(ingredients):\n",
        "    input_text = \"<RECIPE_START> <INPUT_START> <INGREDIENTS_START> \" + ingredients + \" <INPUT_END>\"\n",
        "    input_ids = tokenizer.encode(input_text, add_special_tokens=True, max_length=512, truncation=True, padding='max_length')\n",
        "    return input_ids\n",
        "\n",
        "# Function to generate instructions\n",
        "def generate_instructions(ingredients):\n",
        "    input_ids = preprocess_input(ingredients)\n",
        "    input_tensor = tf.constant([input_ids])\n",
        "\n",
        "    output = model.generate(input_tensor, max_length=1024, do_sample=True, top_k=50, top_p=0.95, num_return_sequences=1)\n",
        "\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    try:\n",
        "        instructions = generated_text.split('<INSTRUCTIONS_START>')[1].split('<INSTRUCTIONS_END>')[0]\n",
        "    except IndexError:\n",
        "        # If the expected tokens are not found, return the entire generated text\n",
        "        instructions = generated_text\n",
        "\n",
        "    return instructions\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olj2joCRVI4g",
        "outputId": "43ffa4d6-3b76-4057-b2f6-4b85333b5aaa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `TFBertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "All model checkpoint layers were used when initializing TFBertLMHeadModel.\n",
            "\n",
            "All the layers of TFBertLMHeadModel were initialized from the model checkpoint at /content/drive/My Drive/generated_data/bert_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertLMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ingredients = \"Green Moong Dal (Whole), Pink Masoor Dal (Split), Arhar dal (Split Toor Dal), White Urad Dal (Split), Chana dal (Bengal Gram Dal), Turmeric powder (Haldi), Oil, Mustard seeds (Rai/ Kadugu), Dry Red Chillies, Bay leaf (tej patta), Lemon juice, Coriander (Dhania) Leaves, Salt, Dry coconut (kopra), Onions, Coriander Powder (Dhania), Cumin powder (Jeera), Kashmiri dry red chillies, Cloves (Laung), Cardamom (Elaichi) Pods/Seeds, Cinnamon Stick (Dalchini), Whole Black Peppercorns, Garlic\"\n",
        "instructions = generate_instructions(ingredients)\n",
        "print(instructions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZITySEEWReqv",
        "outputId": "ca65c8e6-bc51-44bf-bc56-f0ee45c82e26"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "< recipe _ start > < input _ start > < ingredients _ start > green moong dal ( whole ), pink masoor dal ( split ), arhar dal ( split toor dal ), white urad dal ( split ), chana dal ( bengal gram dal ), turmeric powder ( haldi ), oil, mustard seeds ( rai / kadugu ), dry red chillies, bay leaf ( tej patta ), lemon juice, coriander ( dhania ) leaves, salt, dry coconut ( kopra ), onions, coriander powder ( dhania ), cumin powder ( jeera ), kashmiri dry red chillies, cloves ( laung ), cardamom ( elaichi ) pods / seeds, cinnamon stick ( dalchini ), whole black peppercorns, garlic < input _ end > dal a in spices garliceric a minutes addi. you keep keep heat heateric for of the about the into the and seeds chopped for heat. add to sa. well about add add add.. the mix garlic in in of garlic chopped. water the the allow and and and the heat heat heat heat heateric all pressure add to allow the the sa garlic., allowute sa sa garlic and sa sa a powder, onions add add cook sa a done flame the a a and onions..,,r for keep make cook heat the water minutes of the the and the a in in, it of.. garlic the to powderute..ute heat.ute flame ginger all add with add it.. for oil, onions all until add add. to the a until and, sa heat, once and and the it and.. water cooke cook heat sa sa cook, pan a the add once until to add keep water for. for the pan flame powder chopped onions aside heat heat water in about all.. add keep and. is theute with add oil flame and oil heat, sa allow heat heat heat until sa in in in pressure., garlic a a garlic a and add on for chopped.. is, chopped the a,.. the pressure heat minutes add pressure all garlic and sa to to the the anderic with the and heat a onions garlic heat add the the the.. until let in once powder once keep heat heat heat itute the the add for salt sa sa add add the heat the.., and and allow the a the the heat chill heat and add the the the the the add oil add, add, a chill salt. until let let add add add powder aside the the the about aside the cook powder once garlic pan a to chill and the the for allow pan heat for add add add, garlic until. heat sa allow let..., the the the heat turn add.... flame the the the and the the the the the now minutes it chill flame garlic and and to, water heat heat, the the the add. it about pan a the the the the. the for for., on for for to to let the in in in water and the once the. add addute add oil, pan minutes it oil add. heat all it pan flame on turn pan, garlic to first.. garlic a the the addute keep make pressure cooke add flame\n"
          ]
        }
      ]
    }
  ]
}